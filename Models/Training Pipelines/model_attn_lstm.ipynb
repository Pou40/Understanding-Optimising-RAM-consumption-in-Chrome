{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5d0aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  [5.40657439e-05]\n",
      "  ...\n",
      "  [7.56920415e-04]\n",
      "  [0.00000000e+00]\n",
      "  [1.08131488e-04]]\n",
      "\n",
      " [[0.00000000e+00]\n",
      "  [5.40657439e-05]\n",
      "  [0.00000000e+00]\n",
      "  ...\n",
      "  [0.00000000e+00]\n",
      "  [1.08131488e-04]\n",
      "  [0.00000000e+00]]\n",
      "\n",
      " [[5.40657439e-05]\n",
      "  [0.00000000e+00]\n",
      "  [1.62197232e-03]\n",
      "  ...\n",
      "  [1.08131488e-04]\n",
      "  [0.00000000e+00]\n",
      "  [3.29801038e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.18944637e-03]\n",
      "  [7.02854671e-04]\n",
      "  [4.86591696e-04]\n",
      "  ...\n",
      "  [3.29801038e-03]\n",
      "  [1.38408304e-02]\n",
      "  [2.70328720e-04]]\n",
      "\n",
      " [[7.02854671e-04]\n",
      "  [4.86591696e-04]\n",
      "  [0.00000000e+00]\n",
      "  ...\n",
      "  [1.38408304e-02]\n",
      "  [2.70328720e-04]\n",
      "  [9.73183391e-04]]\n",
      "\n",
      " [[4.86591696e-04]\n",
      "  [0.00000000e+00]\n",
      "  [0.00000000e+00]\n",
      "  ...\n",
      "  [2.70328720e-04]\n",
      "  [9.73183391e-04]\n",
      "  [0.00000000e+00]]]\n",
      "[[ 5  3  5 ...  4  3  3]\n",
      " [ 3  5  3 ...  3  3  5]\n",
      " [ 5  3  1 ...  3  5  5]\n",
      " ...\n",
      " [10 10  6 ...  8  8 10]\n",
      " [10  6  7 ...  8 10  3]\n",
      " [ 6  7  3 ... 10  3  5]]\n",
      "[ 5  5  3 ...  3  5 10]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>type</th>\n",
       "      <th>data</th>\n",
       "      <th>delta</th>\n",
       "      <th>event_id</th>\n",
       "      <th>delta_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-03-10 23:22:26</td>\n",
       "      <td>tabSwitched</td>\n",
       "      <td>{'type': 'tabSwitched', 'fromTab': None, 'toTa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-03-10 23:22:26</td>\n",
       "      <td>tabHighlighted</td>\n",
       "      <td>{'type': 'tabHighlighted', 'windowId': 8379253...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-03-10 23:22:27</td>\n",
       "      <td>tabSwitched</td>\n",
       "      <td>{'type': 'tabSwitched', 'fromTab': 837925578, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-03-10 23:22:27</td>\n",
       "      <td>tabHighlighted</td>\n",
       "      <td>{'type': 'tabHighlighted', 'windowId': 8379253...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2025-03-10 23:22:57</td>\n",
       "      <td>tabCreated</td>\n",
       "      <td>{'type': 'tabCreated', 'tabId': 837925613, 'ur...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp            type  \\\n",
       "16 2025-03-10 23:22:26     tabSwitched   \n",
       "17 2025-03-10 23:22:26  tabHighlighted   \n",
       "19 2025-03-10 23:22:27     tabSwitched   \n",
       "20 2025-03-10 23:22:27  tabHighlighted   \n",
       "28 2025-03-10 23:22:57      tabCreated   \n",
       "\n",
       "                                                 data  delta  event_id  \\\n",
       "16  {'type': 'tabSwitched', 'fromTab': None, 'toTa...    0.0         5   \n",
       "17  {'type': 'tabHighlighted', 'windowId': 8379253...    0.0         3   \n",
       "19  {'type': 'tabSwitched', 'fromTab': 837925578, ...    1.0         5   \n",
       "20  {'type': 'tabHighlighted', 'windowId': 8379253...    0.0         3   \n",
       "28  {'type': 'tabCreated', 'tabId': 837925613, 'ur...   30.0         1   \n",
       "\n",
       "    delta_scaled  \n",
       "16      0.000000  \n",
       "17      0.000000  \n",
       "19      0.000054  \n",
       "20      0.000000  \n",
       "28      0.001622  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# ——— Data Loading & Filtering ———\n",
    "df = pd.read_csv(\n",
    "    \"./user_data_qasim_1.csv\",\n",
    "    header=None,\n",
    "    on_bad_lines='skip',\n",
    "    encoding='cp1252'\n",
    ")\n",
    "df.columns = [\"timestamp\", \"type\", \"data\"]\n",
    "df['timestamp'] = pd.to_datetime(\n",
    "    df['timestamp'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "df = df.dropna(subset=['timestamp'])\n",
    "df = df[\n",
    "    (df['type'] != 'memoryUsage') &\n",
    "    (df['type'] != 'tabDuration') &\n",
    "    (df['type'] != 'resourceUsage') &\n",
    "    (df['type'] != 'periodicBrowserStats')\n",
    "]\n",
    "df = df.dropna()\n",
    "df.sort_values(\"timestamp\", inplace=True)\n",
    "\n",
    "# ——— Compute inter-event deltas ———\n",
    "df['delta'] = df['timestamp'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "# ——— Encode and scale ———\n",
    "le = LabelEncoder()\n",
    "df['event_id'] = le.fit_transform(df['type'])\n",
    "scaler = MinMaxScaler()\n",
    "df['delta_scaled'] = scaler.fit_transform(df[['delta']])\n",
    "\n",
    "# ——— Build sequences for Attention-LSTM ———\n",
    "LOOKBACK = 15\n",
    "events = df['event_id'].values\n",
    "deltas = df['delta_scaled'].values\n",
    "\n",
    "X_events, X_deltas, y = [], [], []\n",
    "for i in range(len(events) - LOOKBACK):\n",
    "    X_events.append(events[i:i+LOOKBACK])\n",
    "    X_deltas.append(deltas[i:i+LOOKBACK].reshape(-1, 1))\n",
    "    y.append(events[i+LOOKBACK])\n",
    "X_events = np.array(X_events)\n",
    "X_deltas = np.array(X_deltas)\n",
    "y = np.array(y)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(X_deltas)\n",
    "print(X_events)\n",
    "print(y)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f8cb209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadqasimatiqullah/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, LSTM, Dense,\n",
    "    Concatenate, Softmax, Multiply, Lambda\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class SumOverTime(Layer):\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch, seq_len, hidden)\n",
    "        return K.sum(inputs, axis=1)\n",
    "\n",
    "def build_attn_lstm_model(seq_len, vocab_size, embed_dim, lstm_units, learning_rate):\n",
    "    # 1) Inputs\n",
    "    events_in = Input(shape=(seq_len,), name='events')\n",
    "    deltas_in = Input(shape=(seq_len, 1), name='deltas')\n",
    "\n",
    "    # 2) Embedding + concat\n",
    "    x = Embedding(vocab_size, embed_dim, name='embed')(events_in)\n",
    "    x = Concatenate(name='concat')([x, deltas_in])\n",
    "\n",
    "    # 3) LSTM returns full sequence\n",
    "    x = LSTM(lstm_units, return_sequences=True, name='lstm')(x)\n",
    "\n",
    "    # 4) Bahdanau-style attention scores\n",
    "    scores = Dense(1, activation='tanh', name='attn_score')(x)    # (batch, seq_len, 1)\n",
    "    weights = Softmax(axis=1, name='attn_weights')(scores)        # (batch, seq_len, 1)\n",
    "\n",
    "    # 5) Context vector: weighted sum of all h_t\n",
    "    context = Multiply(name='attn_mul')([x, weights])             # (batch, seq_len, lstm_units)\n",
    "\n",
    "    # — wrap the sum in a Keras layer —\n",
    "    # context = Lambda(lambda t: K.sum(t, axis=1), name='context_sum')(context)  # (batch, lstm_units)\n",
    "\n",
    "    # Sum over the time dimension (axis=1) → shape (batch, lstm_units)\n",
    "    # new\n",
    "    context = SumOverTime(name='context_sum')(context)\n",
    "\n",
    "    # 6) Final prediction\n",
    "    out = Dense(vocab_size, activation='softmax', name='output')(context)\n",
    "\n",
    "    model = Model([events_in, deltas_in], out)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873697b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed=8, units=16, lr=0.001, bs=32, ep=10 → val_loss=1.4513, val_acc=0.3540\n",
      "embed=8, units=16, lr=0.001, bs=32, ep=20 → val_loss=1.4083, val_acc=0.3975\n",
      "embed=8, units=16, lr=0.001, bs=64, ep=10 → val_loss=1.5003, val_acc=0.3463\n",
      "embed=8, units=16, lr=0.001, bs=64, ep=20 → val_loss=1.4449, val_acc=0.3804\n",
      "embed=8, units=16, lr=0.0005, bs=32, ep=10 → val_loss=1.4997, val_acc=0.3385\n",
      "embed=8, units=16, lr=0.0005, bs=32, ep=20 → val_loss=1.4483, val_acc=0.3680\n",
      "embed=8, units=16, lr=0.0005, bs=64, ep=10 → val_loss=1.5328, val_acc=0.3339\n",
      "embed=8, units=16, lr=0.0005, bs=64, ep=20 → val_loss=1.4834, val_acc=0.3587\n",
      "embed=8, units=32, lr=0.001, bs=32, ep=10 → val_loss=1.4007, val_acc=0.4410\n",
      "embed=8, units=32, lr=0.001, bs=32, ep=20 → val_loss=1.3027, val_acc=0.5839\n",
      "embed=8, units=32, lr=0.001, bs=64, ep=10 → val_loss=1.4921, val_acc=0.3929\n",
      "embed=8, units=32, lr=0.001, bs=64, ep=20 → val_loss=1.3934, val_acc=0.4037\n",
      "embed=8, units=32, lr=0.0005, bs=32, ep=10 → val_loss=1.4945, val_acc=0.3509\n",
      "embed=8, units=32, lr=0.0005, bs=32, ep=20 → val_loss=1.4340, val_acc=0.3866\n",
      "embed=8, units=32, lr=0.0005, bs=64, ep=10 → val_loss=1.5111, val_acc=0.3540\n",
      "embed=8, units=32, lr=0.0005, bs=64, ep=20 → val_loss=1.4717, val_acc=0.3354\n",
      "embed=16, units=16, lr=0.001, bs=32, ep=10 → val_loss=1.4398, val_acc=0.4146\n",
      "embed=16, units=16, lr=0.001, bs=32, ep=20 → val_loss=1.3159, val_acc=0.5388\n",
      "embed=16, units=16, lr=0.001, bs=64, ep=10 → val_loss=1.4939, val_acc=0.3292\n",
      "embed=16, units=16, lr=0.001, bs=64, ep=20 → val_loss=1.4334, val_acc=0.3509\n",
      "embed=16, units=16, lr=0.0005, bs=32, ep=10 → val_loss=1.5097, val_acc=0.3339\n",
      "embed=16, units=16, lr=0.0005, bs=32, ep=20 → val_loss=1.4425, val_acc=0.3804\n",
      "embed=16, units=16, lr=0.0005, bs=64, ep=10 → val_loss=1.5187, val_acc=0.3370\n",
      "embed=16, units=16, lr=0.0005, bs=64, ep=20 → val_loss=1.4886, val_acc=0.3649\n",
      "embed=16, units=32, lr=0.001, bs=32, ep=10 → val_loss=1.4072, val_acc=0.3742\n",
      "embed=16, units=32, lr=0.001, bs=32, ep=20 → val_loss=1.1847, val_acc=0.5963\n",
      "embed=16, units=32, lr=0.001, bs=64, ep=10 → val_loss=1.4357, val_acc=0.4425\n",
      "embed=16, units=32, lr=0.001, bs=64, ep=20 → val_loss=1.3881, val_acc=0.4798\n",
      "embed=16, units=32, lr=0.0005, bs=32, ep=10 → val_loss=1.4743, val_acc=0.3525\n",
      "embed=16, units=32, lr=0.0005, bs=32, ep=20 → val_loss=1.4051, val_acc=0.4286\n",
      "embed=16, units=32, lr=0.0005, bs=64, ep=10 → val_loss=1.4983, val_acc=0.3494\n",
      "embed=16, units=32, lr=0.0005, bs=64, ep=20 → val_loss=1.4406, val_acc=0.3478\n",
      "\n",
      "Best configuration: (16, 32, 0.001, 32, 20) with val_loss= 1.1847094297409058\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "param_grid = {\n",
    "    'embed_dim':     [8, 16],\n",
    "    'lstm_units':    [16, 32],\n",
    "    'learning_rate': [1e-3, 5e-4],\n",
    "    'batch_size':    [32, 64],\n",
    "    'epochs':        [10, 20]\n",
    "}\n",
    "\n",
    "best_loss = np.inf\n",
    "best_params = None\n",
    "\n",
    "for embed_dim in param_grid['embed_dim']:\n",
    "    for lstm_units in param_grid['lstm_units']:\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for batch_size in param_grid['batch_size']:\n",
    "                for epochs in param_grid['epochs']:\n",
    "                    K.clear_session()\n",
    "                    model = build_attn_lstm_model(\n",
    "                        LOOKBACK, num_classes,\n",
    "                        embed_dim, lstm_units, lr\n",
    "                    )\n",
    "                    history = model.fit(\n",
    "                        [X_events, X_deltas], y,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    val_loss = history.history['val_loss'][-1]\n",
    "                    val_acc  = history.history['val_accuracy'][-1]\n",
    "                    print(\n",
    "                        f\"embed={embed_dim}, units={lstm_units}, \"\n",
    "                        f\"lr={lr}, bs={batch_size}, ep={epochs} \"\n",
    "                        f\"→ val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\"\n",
    "                    )\n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        best_params = (embed_dim, lstm_units, lr, batch_size, epochs)\n",
    "\n",
    "print(\"\\nBest configuration:\", best_params, \"with val_loss=\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d543c26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3058 - loss: 2.0929\n",
      "Epoch 2/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3402 - loss: 1.5200\n",
      "Epoch 3/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3481 - loss: 1.4949\n",
      "Epoch 4/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3728 - loss: 1.4188\n",
      "Epoch 5/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3708 - loss: 1.4003\n",
      "Epoch 6/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3944 - loss: 1.3810\n",
      "Epoch 7/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3958 - loss: 1.3440\n",
      "Epoch 8/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4403 - loss: 1.3425\n",
      "Epoch 9/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4834 - loss: 1.3458\n",
      "Epoch 10/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4928 - loss: 1.3405\n",
      "Epoch 11/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5794 - loss: 1.2494\n",
      "Epoch 12/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5609 - loss: 1.2439\n",
      "Epoch 13/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5555 - loss: 1.2121\n",
      "Epoch 14/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5963 - loss: 1.1866\n",
      "Epoch 15/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6006 - loss: 1.1556\n",
      "Epoch 16/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6123 - loss: 1.1200\n",
      "Epoch 17/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6021 - loss: 1.1060\n",
      "Epoch 18/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6073 - loss: 1.1040\n",
      "Epoch 19/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6317 - loss: 1.0763\n",
      "Epoch 20/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6112 - loss: 1.0943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full Attention-LSTM as HDF5 to attn_lstm_full_model.h5\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Unpack best hyperparameters\n",
    "embed_dim, lstm_units, lr, batch_size, epochs = best_params\n",
    "\n",
    "# Retrain on full dataset\n",
    "K.clear_session()\n",
    "best_model = build_attn_lstm_model(\n",
    "    seq_len=LOOKBACK,\n",
    "    vocab_size=num_classes,\n",
    "    embed_dim=embed_dim,\n",
    "    lstm_units=lstm_units,\n",
    "    learning_rate=lr\n",
    ")\n",
    "best_model.fit(\n",
    "    [X_events, X_deltas], y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "# After retraining best_model in the notebook:\n",
    "best_model.save('attn_lstm_full_model.h5')\n",
    "print(\"Saved full Attention-LSTM as HDF5 to attn_lstm_full_model.h5\")\n",
    "\n",
    "\n",
    "# Bundle and pickle\n",
    "model_bundle = {\n",
    "    'config':  best_model.get_config(),\n",
    "    'weights': best_model.get_weights(),\n",
    "    'classes': le.classes_.tolist()\n",
    "}\n",
    "\n",
    "# with open('attn_lstm_best_model.pkl', 'wb') as f:\n",
    "#     pickle.dump(model_bundle, f)\n",
    "\n",
    "# print(\"Saved Attention-Augmented LSTM model to attn_lstm_best_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59cda35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Past events sequence:\n",
      "  t-15: event='tabSwitched', Δt=0.000\n",
      "  t-14: event='tabHighlighted', Δt=0.000\n",
      "  t-13: event='tabSwitched', Δt=0.000\n",
      "  t-12: event='tabHighlighted', Δt=0.000\n",
      "  t-11: event='tabCreated', Δt=0.002\n",
      "  t-10: event='tabSwitched', Δt=0.000\n",
      "  t- 9: event='windowFocused', Δt=0.000\n",
      "  t- 8: event='tabHighlighted', Δt=0.000\n",
      "  t- 7: event='tabUpdated', Δt=0.000\n",
      "  t- 6: event='tabTitleChanged', Δt=0.000\n",
      "  t- 5: event='tabSwitched', Δt=0.000\n",
      "  t- 4: event='tabHighlighted', Δt=0.000\n",
      "  t- 3: event='tabRemoved', Δt=0.001\n",
      "  t- 2: event='tabHighlighted', Δt=0.000\n",
      "  t- 1: event='tabHighlighted', Δt=0.000\n",
      "\n",
      "▶ Actual next event: tabSwitched\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\n",
      "▶ Attn-LSTM top-3 predictions:\n",
      "  tabSwitched           p=0.713\n",
      "  tabUpdated            p=0.090\n",
      "  tabTitleChanged       p=0.060\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\n",
      "▶ Attention weights over the past 15 steps:\n",
      "  t-15: w=0.047\n",
      "  t-14: w=0.043\n",
      "  t-13: w=0.043\n",
      "  t-12: w=0.043\n",
      "  t-11: w=0.043\n",
      "  t-10: w=0.043\n",
      "  t- 9: w=0.043\n",
      "  t- 8: w=0.043\n",
      "  t- 7: w=0.043\n",
      "  t- 6: w=0.043\n",
      "  t- 5: w=0.043\n",
      "  t- 4: w=0.043\n",
      "  t- 3: w=0.047\n",
      "  t- 2: w=0.121\n",
      "  t- 1: w=0.314\n"
     ]
    }
   ],
   "source": [
    "# --- Sample extraction ---\n",
    "idx = 0\n",
    "\n",
    "# Raw inputs\n",
    "sample_events = X_events[idx]           # (LOOKBACK,)\n",
    "sample_deltas = X_deltas[idx].flatten() # (LOOKBACK,)\n",
    "\n",
    "# Human-readable past sequence\n",
    "event_names = le.inverse_transform(sample_events)\n",
    "print(\"▶ Past events sequence:\")\n",
    "for i, (e, dt) in enumerate(zip(event_names, sample_deltas)):\n",
    "    print(f\"  t-{LOOKBACK-i:2}: event='{e}', Δt={dt:.3f}\")\n",
    "\n",
    "# Actual next event\n",
    "actual_event = le.inverse_transform([y[idx]])[0]\n",
    "print(f\"\\n▶ Actual next event: {actual_event}\")\n",
    "\n",
    "# # --- T-LSTM Prediction ---\n",
    "# probs_t = best_model.predict(\n",
    "#     [sample_events.reshape(1,-1), sample_deltas.reshape(1,LOOKBACK,1)]\n",
    "# )[0]\n",
    "# top3_t = probs_t.argsort()[-3:][::-1]\n",
    "# print(\"\\n▶ T-LSTM top-3 predictions:\")\n",
    "# for i in top3_t:\n",
    "#     ev, p = le.inverse_transform([i])[0], probs_t[i]\n",
    "#     print(f\"  {ev:<20}  p={p:.3f}\")\n",
    "\n",
    "# --- Attention-LSTM Prediction ---\n",
    "probs_a = best_model.predict(\n",
    "    [sample_events.reshape(1,-1), sample_deltas.reshape(1,LOOKBACK,1)]\n",
    ")[0]\n",
    "top3_a = probs_a.argsort()[-3:][::-1]\n",
    "print(\"\\n▶ Attn-LSTM top-3 predictions:\")\n",
    "for i in top3_a:\n",
    "    ev, p = le.inverse_transform([i])[0], probs_a[i]\n",
    "    print(f\"  {ev:<20}  p={p:.3f}\")\n",
    "\n",
    "# --- Attention Weights ---\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "attn_extractor = Model(\n",
    "    inputs=best_model.input,\n",
    "    outputs=best_model.get_layer('attn_weights').output\n",
    ")\n",
    "attn_out = attn_extractor.predict(\n",
    "    [sample_events.reshape(1,-1), sample_deltas.reshape(1,LOOKBACK,1)]\n",
    ")[0].reshape(-1)\n",
    "\n",
    "print(\"\\n▶ Attention weights over the past 15 steps:\")\n",
    "for i, w in enumerate(attn_out):\n",
    "    print(f\"  t-{LOOKBACK-i:2}: w={w:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7217be15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
