{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f23e75d7",
   "metadata": {},
   "source": [
    "# T-LSTM Hyperparameter Search Notebook\n",
    "This notebook implements a Time-Aware LSTM for predicting the next tab event, with a grid search over hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a324917a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>type</th>\n",
       "      <th>data</th>\n",
       "      <th>delta</th>\n",
       "      <th>event_id</th>\n",
       "      <th>delta_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-03-10 23:22:26</td>\n",
       "      <td>tabSwitched</td>\n",
       "      <td>{'type': 'tabSwitched', 'fromTab': None, 'toTa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-03-10 23:22:26</td>\n",
       "      <td>tabHighlighted</td>\n",
       "      <td>{'type': 'tabHighlighted', 'windowId': 8379253...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-03-10 23:22:27</td>\n",
       "      <td>tabSwitched</td>\n",
       "      <td>{'type': 'tabSwitched', 'fromTab': 837925578, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-03-10 23:22:27</td>\n",
       "      <td>tabHighlighted</td>\n",
       "      <td>{'type': 'tabHighlighted', 'windowId': 8379253...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2025-03-10 23:22:57</td>\n",
       "      <td>tabCreated</td>\n",
       "      <td>{'type': 'tabCreated', 'tabId': 837925613, 'ur...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp            type  \\\n",
       "16 2025-03-10 23:22:26     tabSwitched   \n",
       "17 2025-03-10 23:22:26  tabHighlighted   \n",
       "19 2025-03-10 23:22:27     tabSwitched   \n",
       "20 2025-03-10 23:22:27  tabHighlighted   \n",
       "28 2025-03-10 23:22:57      tabCreated   \n",
       "\n",
       "                                                 data  delta  event_id  \\\n",
       "16  {'type': 'tabSwitched', 'fromTab': None, 'toTa...    0.0         5   \n",
       "17  {'type': 'tabHighlighted', 'windowId': 8379253...    0.0         3   \n",
       "19  {'type': 'tabSwitched', 'fromTab': 837925578, ...    1.0         5   \n",
       "20  {'type': 'tabHighlighted', 'windowId': 8379253...    0.0         3   \n",
       "28  {'type': 'tabCreated', 'tabId': 837925613, 'ur...   30.0         1   \n",
       "\n",
       "    delta_scaled  \n",
       "16      0.000000  \n",
       "17      0.000000  \n",
       "19      0.000054  \n",
       "20      0.000000  \n",
       "28      0.001622  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# ——— Data Loading & Filtering from your snippet ———\n",
    "df = pd.read_csv(\n",
    "    \"./user_data_qasim_1.csv\",\n",
    "    header=None,\n",
    "    on_bad_lines='skip',\n",
    "    encoding=\"cp1252\"\n",
    ")\n",
    "df.columns = [\"timestamp\", \"type\", \"data\"]\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "df = df.dropna(subset=['timestamp'])\n",
    "df = df[\n",
    "    (df['type'] != 'memoryUsage') & \n",
    "    (df['type'] != 'tabDuration') &\n",
    "    (df['type'] != 'resourceUsage') &\n",
    "    (df['type'] != 'periodicBrowserStats')\n",
    "]\n",
    "df = df.dropna()\n",
    "df.sort_values(\"timestamp\", inplace=True)\n",
    "\n",
    "# ——— Compute inter-event deltas ———\n",
    "df['delta'] = df['timestamp'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "# ——— Encode and scale ———\n",
    "le = LabelEncoder()\n",
    "df['event_id'] = le.fit_transform(df['type'])\n",
    "scaler = MinMaxScaler()\n",
    "df['delta_scaled'] = scaler.fit_transform(df[['delta']])\n",
    "\n",
    "# ——— Build sequences for T-LSTM ———\n",
    "LOOKBACK = 15\n",
    "events = df['event_id'].values\n",
    "deltas = df['delta_scaled'].values\n",
    "\n",
    "X_events, X_deltas, y = [], [], []\n",
    "for i in range(len(events) - LOOKBACK):\n",
    "    X_events.append(events[i:i+LOOKBACK])\n",
    "    X_deltas.append(deltas[i:i+LOOKBACK].reshape(-1, 1))\n",
    "    y.append(events[i+LOOKBACK])\n",
    "X_events = np.array(X_events)\n",
    "X_deltas = np.array(X_deltas)\n",
    "y = np.array(y)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "df.head()  # Display the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3744a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadqasimatiqullah/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_tlstm_model(seq_len, vocab_size, embed_dim, lstm_units, learning_rate):\n",
    "    # Inputs\n",
    "    events_in = Input(shape=(seq_len,), name='events')\n",
    "    deltas_in = Input(shape=(seq_len, 1), name='deltas')\n",
    "    # Embedding and concat\n",
    "    x = Embedding(vocab_size, embed_dim, name='embed')(events_in)\n",
    "    x = Concatenate(name='concat')([x, deltas_in])\n",
    "    # LSTM\n",
    "    x = LSTM(lstm_units, name='lstm')(x)\n",
    "    out = Dense(vocab_size, activation='softmax', name='output')(x)\n",
    "    model = Model([events_in, deltas_in], out)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99e1afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed=8, units=16, lr=0.001, bs=32, ep=10 → val_loss=1.1168\n",
      "embed=8, units=16, lr=0.001, bs=32, ep=20 → val_loss=1.0691\n",
      "embed=8, units=16, lr=0.001, bs=64, ep=10 → val_loss=1.2021\n",
      "embed=8, units=16, lr=0.001, bs=64, ep=20 → val_loss=1.0816\n",
      "embed=8, units=16, lr=0.0005, bs=32, ep=10 → val_loss=1.2374\n",
      "embed=8, units=16, lr=0.0005, bs=32, ep=20 → val_loss=1.1495\n",
      "embed=8, units=16, lr=0.0005, bs=64, ep=10 → val_loss=1.2874\n",
      "embed=8, units=16, lr=0.0005, bs=64, ep=20 → val_loss=1.1556\n",
      "embed=8, units=32, lr=0.001, bs=32, ep=10 → val_loss=1.0848\n",
      "embed=8, units=32, lr=0.001, bs=32, ep=20 → val_loss=1.0420\n",
      "embed=8, units=32, lr=0.001, bs=64, ep=10 → val_loss=1.1263\n",
      "embed=8, units=32, lr=0.001, bs=64, ep=20 → val_loss=1.0647\n",
      "embed=8, units=32, lr=0.0005, bs=32, ep=10 → val_loss=1.2061\n",
      "embed=8, units=32, lr=0.0005, bs=32, ep=20 → val_loss=1.0820\n",
      "embed=8, units=32, lr=0.0005, bs=64, ep=10 → val_loss=1.2840\n",
      "embed=8, units=32, lr=0.0005, bs=64, ep=20 → val_loss=1.1213\n",
      "embed=16, units=16, lr=0.001, bs=32, ep=10 → val_loss=1.0924\n",
      "embed=16, units=16, lr=0.001, bs=32, ep=20 → val_loss=1.0432\n",
      "embed=16, units=16, lr=0.001, bs=64, ep=10 → val_loss=1.1245\n",
      "embed=16, units=16, lr=0.001, bs=64, ep=20 → val_loss=1.0594\n",
      "embed=16, units=16, lr=0.0005, bs=32, ep=10 → val_loss=1.1793\n",
      "embed=16, units=16, lr=0.0005, bs=32, ep=20 → val_loss=1.0899\n",
      "embed=16, units=16, lr=0.0005, bs=64, ep=10 → val_loss=1.2677\n",
      "embed=16, units=16, lr=0.0005, bs=64, ep=20 → val_loss=1.1320\n",
      "embed=16, units=32, lr=0.001, bs=32, ep=10 → val_loss=1.0715\n",
      "embed=16, units=32, lr=0.001, bs=32, ep=20 → val_loss=1.0321\n",
      "embed=16, units=32, lr=0.001, bs=64, ep=10 → val_loss=1.0898\n",
      "embed=16, units=32, lr=0.001, bs=64, ep=20 → val_loss=1.0524\n",
      "embed=16, units=32, lr=0.0005, bs=32, ep=10 → val_loss=1.1142\n",
      "embed=16, units=32, lr=0.0005, bs=32, ep=20 → val_loss=1.0612\n",
      "embed=16, units=32, lr=0.0005, bs=64, ep=10 → val_loss=1.2027\n",
      "embed=16, units=32, lr=0.0005, bs=64, ep=20 → val_loss=1.1202\n",
      "\n",
      "Best configuration: (16, 32, 0.001, 32, 20) with val_loss= 1.032078742980957\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Define your grid\n",
    "param_grid = {\n",
    "    'embed_dim':    [8, 16],\n",
    "    'lstm_units':   [16, 32],\n",
    "    'learning_rate':[1e-3, 5e-4],\n",
    "    'batch_size':   [32, 64],\n",
    "    'epochs':       [10, 20]\n",
    "}\n",
    "\n",
    "best_loss = np.inf\n",
    "best_params = None\n",
    "\n",
    "for embed_dim in param_grid['embed_dim']:\n",
    "    for lstm_units in param_grid['lstm_units']:\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for batch_size in param_grid['batch_size']:\n",
    "                for epochs in param_grid['epochs']:\n",
    "                    K.clear_session()\n",
    "                    model = build_tlstm_model(\n",
    "                        LOOKBACK, num_classes,\n",
    "                        embed_dim, lstm_units, lr\n",
    "                    )\n",
    "                    history = model.fit(\n",
    "                        [X_events, X_deltas], y,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    val_loss = history.history['val_loss'][-1]\n",
    "                    print(f\"embed={embed_dim}, units={lstm_units}, \"\n",
    "                          f\"lr={lr}, bs={batch_size}, ep={epochs} \"\n",
    "                          f\"→ val_loss={val_loss:.4f}\")\n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        best_params = (embed_dim, lstm_units, lr, batch_size, epochs)\n",
    "\n",
    "print(\"\\nBest configuration:\", best_params, \"with val_loss=\", best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec825fbe",
   "metadata": {},
   "source": [
    "**Instructions**: Save this notebook as `tlstm_hyperparam_notebook.ipynb` in the same directory as your data and Jupyter file. Then run it to perform hyperparameter search for the Time-Aware LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aceacfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3537 - loss: 2.0557\n",
      "Epoch 2/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4947 - loss: 1.3729\n",
      "Epoch 3/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5958 - loss: 1.1906\n",
      "Epoch 4/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6182 - loss: 1.1288\n",
      "Epoch 5/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6223 - loss: 1.1082\n",
      "Epoch 6/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6343 - loss: 1.0618\n",
      "Epoch 7/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6444 - loss: 1.0526\n",
      "Epoch 8/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6486 - loss: 1.0292\n",
      "Epoch 9/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6509 - loss: 1.0383\n",
      "Epoch 10/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6646 - loss: 0.9982\n",
      "Epoch 11/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6548 - loss: 1.0179\n",
      "Epoch 12/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6478 - loss: 1.0466\n",
      "Epoch 13/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6483 - loss: 1.0004\n",
      "Epoch 14/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6615 - loss: 0.9810\n",
      "Epoch 15/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6636 - loss: 0.9818\n",
      "Epoch 16/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6786 - loss: 0.9564\n",
      "Epoch 17/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6692 - loss: 0.9875\n",
      "Epoch 18/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6635 - loss: 0.9916\n",
      "Epoch 19/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6759 - loss: 0.9643\n",
      "Epoch 20/20\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6819 - loss: 0.9289\n",
      " ======================================= \n",
      "Epoch 1/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3068 - loss: 2.1388 - val_accuracy: 0.3696 - val_loss: 1.5233\n",
      "Epoch 2/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4585 - loss: 1.4522 - val_accuracy: 0.5637 - val_loss: 1.3471\n",
      "Epoch 3/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5859 - loss: 1.2218 - val_accuracy: 0.5932 - val_loss: 1.2296\n",
      "Epoch 4/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6061 - loss: 1.1301 - val_accuracy: 0.6056 - val_loss: 1.1808\n",
      "Epoch 5/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6046 - loss: 1.1536 - val_accuracy: 0.6180 - val_loss: 1.1468\n",
      "Epoch 6/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6347 - loss: 1.0709 - val_accuracy: 0.6289 - val_loss: 1.1311\n",
      "Epoch 7/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6217 - loss: 1.0735 - val_accuracy: 0.6382 - val_loss: 1.0943\n",
      "Epoch 8/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6403 - loss: 1.0372 - val_accuracy: 0.6351 - val_loss: 1.0787\n",
      "Epoch 9/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6458 - loss: 1.0315 - val_accuracy: 0.6491 - val_loss: 1.0774\n",
      "Epoch 10/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6499 - loss: 1.0355 - val_accuracy: 0.6522 - val_loss: 1.0624\n",
      "Epoch 11/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6602 - loss: 1.0052 - val_accuracy: 0.6506 - val_loss: 1.0519\n",
      "Epoch 12/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6568 - loss: 1.0264 - val_accuracy: 0.6444 - val_loss: 1.0561\n",
      "Epoch 13/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6655 - loss: 1.0010 - val_accuracy: 0.6475 - val_loss: 1.0464\n",
      "Epoch 14/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6602 - loss: 0.9980 - val_accuracy: 0.6522 - val_loss: 1.0487\n",
      "Epoch 15/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6701 - loss: 0.9650 - val_accuracy: 0.6475 - val_loss: 1.0359\n",
      "Epoch 16/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6724 - loss: 0.9416 - val_accuracy: 0.6553 - val_loss: 1.0361\n",
      "Epoch 17/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6828 - loss: 0.9597 - val_accuracy: 0.6429 - val_loss: 1.0343\n",
      "Epoch 18/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6800 - loss: 0.9222 - val_accuracy: 0.6398 - val_loss: 1.0412\n",
      "Epoch 19/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6676 - loss: 0.9427 - val_accuracy: 0.6553 - val_loss: 1.0239\n",
      "Epoch 20/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6516 - loss: 0.9676 - val_accuracy: 0.6537 - val_loss: 1.0229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# 1) Unpack your best hyperparameters\n",
    "embed_dim, lstm_units, lr, batch_size, epochs = best_params\n",
    "\n",
    "# 2) (Re)build and retrain on the full dataset\n",
    "K.clear_session()\n",
    "best_model = build_tlstm_model(\n",
    "    seq_len=LOOKBACK,\n",
    "    vocab_size=num_classes,\n",
    "    embed_dim=embed_dim,\n",
    "    lstm_units=lstm_units,\n",
    "    learning_rate=lr\n",
    ")\n",
    "best_model.fit(\n",
    "    [X_events, X_deltas],\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\" ======================================= \")\n",
    "\n",
    "#3) Run with Validation Level\n",
    "K.clear_session()\n",
    "best_model = build_tlstm_model(\n",
    "    seq_len=LOOKBACK,\n",
    "    vocab_size=num_classes,\n",
    "    embed_dim=embed_dim,\n",
    "    lstm_units=lstm_units,\n",
    "    learning_rate=lr\n",
    ")\n",
    "history = best_model.fit(\n",
    "    [X_events, X_deltas],\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,   # reserve 20% of data for validation\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "best_model.save('tlstm_full.h5')\n",
    "\n",
    "# 3) Bundle config, weights, and your LabelEncoder classes\n",
    "model_bundle = {\n",
    "    'config':  best_model.get_config(),\n",
    "    'weights': best_model.get_weights(),\n",
    "    'classes': le.classes_.tolist()\n",
    "}\n",
    "\n",
    "# # 4) Write out the pickle\n",
    "# with open('tlstm_best_model.pkl', 'wb') as f:\n",
    "#     pickle.dump(model_bundle, f)\n",
    "\n",
    "# print(\"Saved Time-Aware LSTM model to tlstm_best_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb2f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Past events sequence:\n",
      "  t-15: event='tabSwitched', Δt=0.000\n",
      "  t-14: event='tabHighlighted', Δt=0.000\n",
      "  t-13: event='tabSwitched', Δt=0.000\n",
      "  t-12: event='tabHighlighted', Δt=0.000\n",
      "  t-11: event='tabCreated', Δt=0.002\n",
      "  t-10: event='tabSwitched', Δt=0.000\n",
      "  t-9: event='windowFocused', Δt=0.000\n",
      "  t-8: event='tabHighlighted', Δt=0.000\n",
      "  t-7: event='tabUpdated', Δt=0.000\n",
      "  t-6: event='tabTitleChanged', Δt=0.000\n",
      "  t-5: event='tabSwitched', Δt=0.000\n",
      "  t-4: event='tabHighlighted', Δt=0.000\n",
      "  t-3: event='tabRemoved', Δt=0.001\n",
      "  t-2: event='tabHighlighted', Δt=0.000\n",
      "  t-1: event='tabHighlighted', Δt=0.000\n"
     ]
    }
   ],
   "source": [
    "# pick the first example from your data\n",
    "idx = 0\n",
    "\n",
    "# raw inputs\n",
    "sample_events = X_events[idx]         # shape (LOOKBACK,)\n",
    "sample_deltas = X_deltas[idx].flatten()  # shape (LOOKBACK,)\n",
    "\n",
    "# decode to human‐readable event names\n",
    "event_names = le.inverse_transform(sample_events)\n",
    "\n",
    "print(\"▶ Past events sequence:\")\n",
    "for i, (e, dt) in enumerate(zip(event_names, sample_deltas)):\n",
    "    print(f\"  t-{LOOKBACK-i}: event='{e}', Δt={dt:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67b9b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "▶ T-LSTM top-3 predictions:\n",
      "  tabSwitched           p=0.827\n",
      "  tabUpdated            p=0.053\n",
      "  windowFocused         p=0.039\n"
     ]
    }
   ],
   "source": [
    "# get T-LSTM prediction distribution\n",
    "probs_t = best_model.predict(\n",
    "    [sample_events.reshape(1,-1), sample_deltas.reshape(1,LOOKBACK,1)]\n",
    ")[0]\n",
    "\n",
    "# top-3 events\n",
    "top3_idx = probs_t.argsort()[-3:][::-1]\n",
    "top3 = list(zip(\n",
    "    le.inverse_transform(top3_idx),\n",
    "    probs_t[top3_idx]\n",
    "))\n",
    "\n",
    "print(\"▶ T-LSTM top-3 predictions:\")\n",
    "for event, p in top3:\n",
    "    print(f\"  {event:<20}  p={p:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
